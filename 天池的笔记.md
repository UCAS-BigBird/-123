# <center>这是一个标题</center>>

输出最终结果  

~~~python
import pandas as pd
pd.DataFrame(final).to_csv(r'C:h\Users\UCAS_BigBird\Desktop\ouy							  ue.txt',index=False,header=False)
~~~

==metrics  参数坐标==

~~~python
param2 = {'silent':True
          ,'obj':'reg:linear'
          ,"max_depth":2
          ,'eta':0.05
          ,'gamma':2
          ,"nfold":5}
bst=xgb.train(params=param2,num_boost_round=200,dtrain=dfull)

dtest = xgb.DMatrix(test)
q=bst.predict(dtest)

import pandas as pd
pd.DataFrame(q).to_csv(r'C:\Users\UCAS_BigBird\Desktop\ouyue.txt',index=False,
                       header=False)
~~~

==上方代码证明如果顺利使用xgboost训练完的结果==



==下方代码给出了xgboost 调试的经验==

~~~python
dfull = xgb.DMatrix(train,y)
param1 = {'silent':True #并非默认
          ,'obj':'reg:linear' #并非默认
          ,"subsample":1  #随机抽样的时候抽取的样本比例
          ,"max_depth":6  #最大深度
          ,"eta":0.3      #树的迭代速率
          ,"gamma":0      #gamma是对梯度提升树影响最大的参数之一
          ,"lambda":1     #L2正则项
          ,"alpha":0      #L1正则项
          ,"colsample_bytree":1
          ,"colsample_bylevel":1
          ,"colsample_bynode":1
          ,"nfold":5}
num_round = 200
cvresult1 = xgb.cv(param1, dfull, num_round)
print('完全是OK的！')

fig,ax = plt.subplots(1,figsize=(15,8))
#ax.set_ylim(top=5)
ax.grid()
ax.plot(range(1,201),cvresult1.iloc[:,0],c="red",label="train,original")
ax.plot(range(1,201),cvresult1.iloc[:,2],c="orange",label="test,original")
ax.legend(fontsize="xx-large")
plt.show()
~~~

